# Summarization

## Project Overview

This repository contains the implementation of a research assignment focused on developing efficient summarization methodologies. The goal was to build a robust pipeline capable of ingesting raw, noisy web scrapes and producing concise, factual summaries without relying on expensive abstractive LLM calls.

The solution features a custom **Map-Reduce** architecture to handle large documents and compares three distinct algorithmic approaches: a statistical baseline (**KL-SUM**), a semantic graph method (**TextRank**), and a structure-aware heuristic method (**Weighted TextRank**).

## Methodologies


### Algorithms
* **KL-SUM (Lite):** A greedy, statistical approach that selects sentences to minimize the Kullback-Leibler divergence between the summary's vocabulary distribution and the original document. It serves as a high-speed, low-latency baseline.
* **TextRank (Semantic):** An unsupervised graph-based algorithm. Sentences are nodes in a graph, and edges are weighted by semantic cosine similarity (using `SentenceTransformers` embeddings). PageRank is applied to identify the most "central" sentences.
* **Weighted TextRank (Structural):** An enhancement of TextRank that utilizes the Markdown structure. Sentences within major headers (H1, H2, H3) receive heuristic weight boosts in the adjacency matrix, guiding the algorithm to prioritize structurally significant content.

## Benchmark Results

Evaluation was conducted using **ROUGE** (N-gram overlap) and **BERTScore** (Semantic similarity) against "Gold Standard" summaries generated by a high-intelligence LLM.

### Overall Performance
*Bold indicates the highest score.*
| Metric | Measure | KL-SUM | Weighted TextRank | TextRank | Baseline |
| :--- | :--- | :---: | :---: | :---: | :---: |
| **BERTScore** | **Precision** | 0.6432 | **0.6557** | **0.6557** | 0.6447 |
| | **Recall** | **0.6684** | 0.6366 | 0.6366 | 0.6349 |
| | **F1-Score** | **0.6546** | 0.6451 | 0.6451 | 0.6387 |
| **ROUGE-1** | **Precision** | 0.3680 | **0.3919** | 0.3849 | 0.3835 |
| | **Recall** | **0.4036** | 0.2842 | 0.2853 | 0.2903 |
| | **F1-Score** | **0.3528** | 0.3004 | 0.2974 | 0.3001 |
| **ROUGE-2** | **Precision** | 0.1203 | **0.1268** | 0.1249 | 0.1190 |
| | **Recall** | **0.1484** | 0.1012 | 0.1013 | 0.1007 |
| | **F1-Score** | **0.1229** | 0.1016 | 0.1009 | 0.0985 |
| **ROUGE-3** | **Precision** | 0.0587 | **0.0604** | 0.0596 | 0.0557 |
| | **Recall** | **0.0756** | 0.0518 | 0.0513 | 0.0507 |
| | **F1-Score** | **0.0614** | 0.0498 | 0.0494 | 0.0479 |
| **ROUGE-L** | **Precision** | 0.2147 | **0.2256** | 0.2226 | 0.2230 |
| | **Recall** | **0.2414** | 0.1666 | 0.1667 | 0.1844 |
| | **F1-Score** | **0.2040** | 0.1722 | 0.1703 | 0.1806 |

---
### Performance by Document Length
*Comparison of primary semantic metric (BERTScore F1) and n-gram metric.*

| Content Size | Metric | KL-SUM | Weighted TextRank | TextRank | Baseline |
| :--- | :--- | :---: | :---: | :---: | :---: |
| **Short** | **BERTScore** | **0.7052** | 0.6914 | 0.6914 | 0.6776 |
| *(< 33rd percentile)* | **ROUGE-1** | **0.4631** | 0.4137 | 0.4099 | 0.3819 |
| | **ROUGE-2** | **0.1999** | 0.1733 | 0.1741 | 0.1582 |
| | **ROUGE-3** | **0.1089** | 0.0930 | 0.0941 | 0.0871 |
| | **ROUGE-L** | **0.3070** | 0.2510 | 0.2488 | 0.2537 |
| **Medium** | **BERTScore** | **0.6663** | 0.6503 | 0.6503 | 0.6485 |
| *(33rd - 67th)* | **ROUGE-1** | **0.3604** | 0.2891 | 0.2825 | 0.2963 |
| | **ROUGE-2** | **0.1140** | 0.0864 | 0.0827 | 0.0848 |
| | **ROUGE-3** | **0.0543** | 0.0400 | 0.0376 | 0.0362 |
| | **ROUGE-L** | **0.1880** | 0.1557 | 0.1537 | 0.1658 |
| **Long** | **BERTScore** | 0.5920 | **0.5936** | **0.5936** | 0.5896 |
| *(> 67th percentile)* | **ROUGE-1** | **0.2349** | 0.1985 | 0.2001 | 0.2222 |
| | **ROUGE-2** | **0.0549** | 0.0454 | 0.0461 | 0.0528 |
| | **ROUGE-3** | **0.0213** | 0.0166 | 0.0167 | 0.0206 |
| | **ROUGE-L** | 0.1173 | 0.1102 | 0.1087 | **0.1228** |

